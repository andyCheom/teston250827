import os
import json
import base64
import logging
import re
import mimetypes
import asyncio
import hashlib
from functools import lru_cache
from typing import Dict, Any, List, Tuple, Optional
from fastapi import FastAPI, File, UploadFile, HTTPException, Form
from fastapi.responses import JSONResponse, FileResponse, StreamingResponse
from fastapi.staticfiles import StaticFiles
import google.auth
import google.auth.transport.requests
from google.cloud import storage, spanner
import requests, markdown
from requests import exceptions as requests_exceptions
from dotenv import load_dotenv
from google.auth.exceptions import DefaultCredentialsError
from google.oauth2 import service_account
import aiohttp

app = FastAPI(static_files_directory="public", title="Gemini RAG Chatbot API", version="1.0.0")
load_dotenv()

class Config:
    def get_env(name: str) -> str:
        if name not in os.environ:
            raise EnvironmentError(f"âŒ í™˜ê²½ë³€ìˆ˜ '{name}'ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return os.environ[name]

    PROJECT_ID = get_env('PROJECT_ID')
    LOCATION_ID = get_env('LOCATION_ID')
    MODEL_ID = get_env('MODEL_ID')
    DATASTORE_ID = get_env('DATASTORE_ID')
    DATASTORE_LOCATION = get_env('DATASTORE_LOCATION')
    SYSTEM_PROMPT_PATH = get_env('SYSTEM_PROMPT_PATH')
    SPANNER_INSTANCE_ID = get_env('SPANNER_INSTANCE_ID')
    SPANNER_DATABASE_ID = get_env('SPANNER_DATABASE_ID')
    SPANNER_TABLE_NAME = get_env('SPANNER_TABLE_NAME')

    API_ENDPOINT = f"https://{LOCATION_ID}-aiplatform.googleapis.com"
    MODEL_ENDPOINT_URL = f"{API_ENDPOINT}/v1/projects/{PROJECT_ID}/locations/{LOCATION_ID}/publishers/google/models/{MODEL_ID}:generateContent"
    DATASTORE_PATH = f"projects/{PROJECT_ID}/locations/{DATASTORE_LOCATION}/collections/default_collection/dataStores/{DATASTORE_ID}"

    with open(SYSTEM_PROMPT_PATH, "r", encoding="utf-8") as f:
        SYSTEM_INSTRUCTION = f.read()

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# ë©”ëª¨ë¦¬ ìºì‹œ (ê°„ë‹¨í•œ êµ¬í˜„)
class MemoryCache:
    def __init__(self, max_size: int = 1000):
        self.cache = {}
        self.max_size = max_size
        
    def get(self, key: str):
        return self.cache.get(key)
        
    def set(self, key: str, value: Any, ttl_seconds: int = 3600):
        if len(self.cache) >= self.max_size:
            # LRU ë°©ì‹ìœ¼ë¡œ ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
            oldest_key = next(iter(self.cache))
            del self.cache[oldest_key]
        
        import time
        self.cache[key] = {
            'value': value,
            'expires': time.time() + ttl_seconds
        }
    
    def is_valid(self, key: str) -> bool:
        import time
        if key not in self.cache:
            return False
        return time.time() < self.cache[key]['expires']

memory_cache = MemoryCache()

# DB ì—°ê²° í’€ë§ 
@lru_cache(maxsize=1)
def get_database_connection():
    """Spanner ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì„ ìºì‹œí•˜ì—¬ ì¬ì‚¬ìš©"""
    instance = spanner_client.instance(Config.SPANNER_INSTANCE_ID)
    return instance.database(Config.SPANNER_DATABASE_ID)

SERVICE_ACCOUNT_PATH = "keys/cheom-kdb-test1-faf5cf87a1fd.json"
try:
    credentials = service_account.Credentials.from_service_account_file(
        SERVICE_ACCOUNT_PATH,
        scopes=["https://www.googleapis.com/auth/cloud-platform"]
    )
    project_id = credentials.project_id
    storage_client = storage.Client(credentials=credentials, project=project_id)
    spanner_client = spanner.Client(credentials=credentials, project=project_id)
    logger.info(f"âœ… ì¸ì¦ ì„±ê³µ - project_id: {project_id}")
except Exception as e:
    logger.critical("âŒ ì¸ì¦ ì˜¤ë¥˜", exc_info=True)
    credentials = None
    storage_client = None
    spanner_client = None

class VertexAIAPIError(Exception):
    def __init__(self, message: str, status_code: int, error_body: str):
        super().__init__(message)
        self.status_code = status_code
        self.error_body = error_body

def get_cache_key(prefix: str, *args) -> str:
    """ìºì‹œ í‚¤ ìƒì„±"""
    combined = f"{prefix}:{'|'.join(str(arg) for arg in args)}"
    return hashlib.md5(combined.encode()).hexdigest()

def query_spanner_triples(user_prompt: str) -> List[str]:
    # ìºì‹œ í™•ì¸
    cache_key = get_cache_key("spanner_triples", user_prompt)
    if memory_cache.is_valid(cache_key):
        cached_result = memory_cache.get(cache_key)['value']
        logger.info(f"ìºì‹œì—ì„œ Triple ê²€ìƒ‰ ê²°ê³¼ ë°˜í™˜: {len(cached_result)}ê±´")
        return cached_result
    
    try:
        logger.info(json.dumps({
            "stage": "spanner_query_start",
            "input": user_prompt
        }))

        database = get_database_connection()
        
        # í‚¤ì›Œë“œ ë¶„í•´í•˜ì—¬ ë” ì •í™•í•œ ê²€ìƒ‰
        keywords = user_prompt.split()
        conditions = []
        params = {}
        param_types = {}
        
        for i, keyword in enumerate(keywords):
            param_name = f"keyword_{i}"
            conditions.extend([
                f"LOWER(subject) LIKE @{param_name}",
                f"LOWER(predicate) LIKE @{param_name}",
                f"LOWER(object) LIKE @{param_name}"
            ])
            params[param_name] = f"%{keyword.lower()}%"
            param_types[param_name] = spanner.param_types.STRING
        
        where_clause = " OR ".join(conditions) if conditions else "1=1"
        query = f"""
        SELECT subject, predicate, object FROM `{Config.SPANNER_TABLE_NAME}`
        WHERE {where_clause}
        LIMIT 50
        """

        with database.snapshot() as snapshot:
            results = snapshot.execute_sql(query, params=params, param_types=param_types)
            triples = [f"{row[0]} {row[1]} {row[2]}" for row in results]

        logger.info(json.dumps({
            "stage": "spanner_query_success",
            "input": user_prompt,
            "result_count": len(triples),
            "results": triples
        }))

        # ê²°ê³¼ ìºì‹œ ì €ì¥ (1ì‹œê°„)
        memory_cache.set(cache_key, triples, 3600)
        
        return triples

    except Exception as e:
        logger.error(json.dumps({
            "stage": "spanner_query_error",
            "input": user_prompt,
            "error": str(e)
        }), exc_info=True)
        return []

def query_spanner_by_triple(subject: str, predicate: str, object_: str) -> List[str]:
    try:
        logger.info(json.dumps({
            "stage": "spanner_triple_query_start",
            "subject": subject,
            "predicate": predicate,
            "object": object_
        }))

        database = get_database_connection()
        
        # ê° triple ìš”ì†Œì— ëŒ€í•´ ìœ ì—°í•œ ê²€ìƒ‰
        conditions = []
        params = {}
        param_types = {}
        
        if subject and subject.strip():
            conditions.append("LOWER(subject) LIKE @subject_param")
            params["subject_param"] = f"%{subject.lower().strip()}%"
            param_types["subject_param"] = spanner.param_types.STRING
            
        if predicate and predicate.strip():
            conditions.append("LOWER(predicate) LIKE @predicate_param")
            params["predicate_param"] = f"%{predicate.lower().strip()}%"
            param_types["predicate_param"] = spanner.param_types.STRING
            
        if object_ and object_.strip():
            conditions.append("LOWER(object) LIKE @object_param")
            params["object_param"] = f"%{object_.lower().strip()}%"
            param_types["object_param"] = spanner.param_types.STRING
        
        if not conditions:
            logger.warning("ëª¨ë“  triple ìš”ì†Œê°€ ë¹„ì–´ìˆìŒ")
            return []
            
        where_clause = " OR ".join(conditions)
        query = f"""
        SELECT subject, predicate, object FROM `{Config.SPANNER_TABLE_NAME}`
        WHERE {where_clause}
        LIMIT 30
        """

        with database.snapshot() as snapshot:
            results = snapshot.execute_sql(query, params=params, param_types=param_types)
            triples = [f"{row[0]} {row[1]} {row[2]}" for row in results]

        logger.info(json.dumps({
            "stage": "spanner_triple_query_success",
            "subject": subject,
            "predicate": predicate,
            "object": object_,
            "result_count": len(triples),
            "results": triples
        }))

        return triples

    except Exception as e:
        logger.error(json.dumps({
            "stage": "spanner_triple_query_error",
            "subject": subject,
            "predicate": predicate,
            "object": object_,
            "error": str(e)
        }), exc_info=True)
        return []

async def extract_triple_from_prompt(user_prompt: str) -> Tuple[str, str, str]:
    prompt = f"""
ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ í•µì‹¬ í‚¤ì›Œë“œë¥¼ (subject, predicate, object) tripleë¡œ ì¶”ì¶œí•´ì¤˜.

ì§ˆë¬¸: "{user_prompt}"

ì¶”ì¶œ ê·œì¹™:
- subject: ì§ˆë¬¸ì˜ ì£¼ìš” ëŒ€ìƒ (ì œí’ˆëª…, ê¸°ëŠ¥ëª… ë“±)
- predicate: ê´€ê³„ë‚˜ ë™ì‘ (ì‚¬ìš©ë²•, ì„¤ì •, ë¬¸ì œí•´ê²° ë“±)  
- object: êµ¬ì²´ì  ì†ì„±ì´ë‚˜ ê²°ê³¼

ì‘ë‹µ í˜•ì‹: subject=..., predicate=..., object=...

"""
    payload = {
        "contents": [{"role": "user", "parts": [{"text": prompt}]}],
        "generationConfig": {"temperature": 0.1, "maxOutputTokens": 200}
    }

    response = await _call_vertex_api(payload)
    text = response["candidates"][0]["content"]["parts"][0]["text"]

    # ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ì¶”ì¶œ
    match = re.search(r"subject\s*=\s*(.+?),\s*predicate\s*=\s*(.+?),\s*object\s*=\s*(.*)", text)
    if match:
        subject = match.group(1).strip()
        predicate = match.group(2).strip() 
        object_ = match.group(3).strip()
        
        # ë¬´ê´€í•œ ì§ˆë¬¸ ì²´í¬
        if subject == "IRRELEVANT":
            raise ValueError("ì§ˆë¬¸ì´ ì²˜ìŒì„œë¹„ìŠ¤ì™€ ë¬´ê´€í•¨")
            
        return subject, predicate, object_
    else:
        raise ValueError("Triple ì¶”ì¶œ ì‹¤íŒ¨: " + text)

async def validate_response_relevance(user_prompt: str, response: str) -> bool:
    """ì‘ë‹µì´ ì§ˆë¬¸ê³¼ ì—°ê´€ì„±ì´ ìˆëŠ”ì§€ ê²€ì¦"""
    validation_prompt = f"""
ì‚¬ìš©ì ì§ˆë¬¸: "{user_prompt}"
AI ì‘ë‹µ: "{response[:500]}..."

ìœ„ ì‘ë‹µì´ ì§ˆë¬¸ì— ì ì ˆíˆ ë‹µí•˜ê³  ìˆëŠ”ì§€ íŒë‹¨í•´ì¤˜.

íŒë‹¨ ê¸°ì¤€:
1. ì§ˆë¬¸ì˜ í•µì‹¬ ì˜ë„ì— ë¶€í•©í•˜ëŠ”ê°€?
2. êµ¬ì²´ì ì´ê³  ìœ ìš©í•œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ”ê°€?
3. "ì£„ì†¡í•©ë‹ˆë‹¤", "ë‹µë³€ë“œë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" ê°™ì€ íšŒí”¼ ë‹µë³€ì´ ì•„ë‹Œê°€?

ì‘ë‹µ: YES ë˜ëŠ” NO
"""
    
    payload = {
        "contents": [{"role": "user", "parts": [{"text": validation_prompt}]}],
        "generationConfig": {"temperature": 0.0, "maxOutputTokens": 10}
    }
    
    try:
        response = await _call_vertex_api(payload)
        result = response["candidates"][0]["content"]["parts"][0]["text"].strip()
        return result.upper() == "YES"
    except:
        return True  # ê²€ì¦ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ì ìœ¼ë¡œ í†µê³¼




async def _build_vertex_payload(
    user_prompt: str,
    conversation_history: List[Dict[str, Any]],
    image_file: Optional[UploadFile],
    preloaded_triples: Optional[List[str]] = None
) -> Tuple[Dict[str, Any], List[Dict[str, Any]]]:
    user_content_parts = []
    if image_file:
        image_base64 = base64.b64encode(await image_file.read()).decode('utf-8')
        user_content_parts.append({"inlineData": {"mimeType": image_file.content_type, "data": image_base64}})

    if user_prompt:
        user_content_parts.append({"text": user_prompt})

    current_contents = conversation_history + [{"role": "user", "parts": user_content_parts}]

    # ğŸ§  Triple grounding: ë¯¸ë¦¬ ë°›ì•„ì˜¨ ê²Œ ìˆìœ¼ë©´ ì“°ê³ , ì—†ìœ¼ë©´ ì¶”ì¶œ
    if preloaded_triples is not None:
        triples = preloaded_triples
    else:
        try:
            subject, predicate, object_ = await extract_triple_from_prompt(user_prompt)
            triples = query_spanner_by_triple(subject, predicate, object_)
        except Exception as e:
            logger.warning(f"Triple ì¶”ì¶œ ë˜ëŠ” ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            triples = []

    # ğŸ“ grounding ë‚´ìš© system prompt ì•ì— ì‚½ì…
    if triples:
        triple_text = "\n".join(triples)
        current_contents.insert(0, {
            "role": "user",
            "parts": [{"text": f"[Spanner Triple Grounding]\n{triple_text}"}]
        })

    payload = {
        "systemInstruction": {"parts": [{"text": Config.SYSTEM_INSTRUCTION}]},
        "contents": current_contents,
        "tools": [{"retrieval": {"vertexAiSearch": {"datastore": Config.DATASTORE_PATH}}}],
        "generationConfig": {
            "temperature": 0,
            "maxOutputTokens": 8192,
            "topP": 0.3
        }
    }

    return payload, current_contents


# ê³µí†µ ì„¸ì…˜ê³¼ í—¤ë” ìºì‹±
_shared_session = None
_cached_headers = None
_headers_cache_time = 0

async def get_shared_session():
    global _shared_session
    if _shared_session is None or _shared_session.closed:
        # ì—°ê²° í’€ ìµœì í™” ì„¤ì •
        connector = aiohttp.TCPConnector(
            limit=100,  # ìµœëŒ€ ì—°ê²° ìˆ˜
            limit_per_host=20,  # í˜¸ìŠ¤íŠ¸ë‹¹ ìµœëŒ€ ì—°ê²° ìˆ˜
            keepalive_timeout=30,  # Keep-alive íƒ€ì„ì•„ì›ƒ
            enable_cleanup_closed=True
        )
        timeout = aiohttp.ClientTimeout(total=300, connect=10)
        _shared_session = aiohttp.ClientSession(connector=connector, timeout=timeout)
    return _shared_session

async def get_cached_headers():
    global _cached_headers, _headers_cache_time
    import time
    
    # í—¤ë”ë¥¼ 5ë¶„ê°„ ìºì‹œ
    if _cached_headers is None or time.time() - _headers_cache_time > 300:
        if not credentials:
            raise ConnectionAbortedError("Server authentication is not configured.")
        
        auth_req = google.auth.transport.requests.Request()
        credentials.refresh(auth_req)
        
        _cached_headers = {
            'Authorization': f'Bearer {credentials.token}',
            'Content-Type': 'application/json; charset=utf-8'
        }
        _headers_cache_time = time.time()
    
    return _cached_headers

async def _call_vertex_api(payload: Dict[str, Any]) -> Dict[str, Any]:
    session = await get_shared_session()
    headers = await get_cached_headers()

    async with session.post(Config.MODEL_ENDPOINT_URL, headers=headers, json=payload) as response:
        if not response.ok:
            error_body = await response.text()
            raise VertexAIAPIError(f"HTTP error {response.status}", response.status, error_body)
        return await response.json()


async def _build_triple_only_payload(user_prompt: str, triples: List[str]) -> Dict[str, Any]:
    triple_text = "\n".join(triples) if triples else "ê´€ë ¨ëœ triple ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    instruction = f"""ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì œê³µëœ triple ì •ë³´ë§Œìœ¼ë¡œ ë‹µë³€ì„ ì‘ì„±í•˜ëŠ” AIì…ë‹ˆë‹¤.
ì•„ë˜ëŠ” triple ì •ë³´ì…ë‹ˆë‹¤:
{triple_text}
ì‚¬ìš©ì ì§ˆë¬¸: {user_prompt}"""
    payload = {
        "contents": [{"role": "user", "parts": [{"text": instruction}]}],
        "generationConfig": {"temperature": 0, "maxOutputTokens": 8192, "topP": 0.8}
    }
    return payload

async def _build_summary_payload(triple_answer: str, vertex_answer: str, user_prompt: str) -> Dict[str, Any]:
    summary_prompt = f"""ì‚¬ìš©ìì˜ ì§ˆë¬¸: {user_prompt}

[Spanner Triple ê¸°ë°˜ ì‘ë‹µ]
{triple_answer}

[Vertex AI Search ê¸°ë°˜ ì‘ë‹µ]
{vertex_answer}

ìœ„ ë‘ ì‘ë‹µì„ ì°¸ê³ í•˜ì—¬ ìµœì¢… ì‘ë‹µì„ ìƒì„±í•˜ì„¸ìš”."""
    return {
        "contents": [{"role": "user", "parts": [{"text": summary_prompt}]}],
        "generationConfig": {"temperature": 0, "maxOutputTokens": 16192, "topP": 0.8}
    }

@app.post('/api/generate')
async def generate_content(userPrompt: str = Form(""), conversationHistory: str = Form("[]"), imageFile: Optional[UploadFile] = File(None)):
    if not credentials:
        raise HTTPException(status_code=503, detail="ì„œë²„ ì¸ì¦ ì‹¤íŒ¨")

    try:
        conversation_history = json.loads(conversationHistory)

        # ğŸ”¹ Step 1: Triple ê²€ìƒ‰ ë° ê¸°ë°˜ ì‘ë‹µ
        triples = query_spanner_triples(userPrompt)
        
        # Tripleì´ ì—†ìœ¼ë©´ ì¶”ì¶œí•˜ì—¬ ë‹¤ì‹œ ê²€ìƒ‰ ì‹œë„
        if not triples:
            try:
                subject, predicate, object_ = await extract_triple_from_prompt(userPrompt)
                triples = query_spanner_by_triple(subject, predicate, object_)
                logger.info(f"Fallback triple ê²€ìƒ‰ ê²°ê³¼: {len(triples)}ê±´")
            except Exception as e:
                logger.warning(f"Fallback triple ê²€ìƒ‰ ì‹¤íŒ¨: {e}")

        # ğŸš€ Step 1&2: Triple ê¸°ë°˜ ì‘ë‹µê³¼ Vertex AI ê²€ìƒ‰ì„ ë³‘ë ¬ ì²˜ë¦¬
        triple_payload = await _build_triple_only_payload(userPrompt, triples)
        full_payload, full_history = await _build_vertex_payload(userPrompt, conversation_history, imageFile, preloaded_triples=triples)
        
        # ë³‘ë ¬ API í˜¸ì¶œë¡œ ì†ë„ 2ë°° í–¥ìƒ
        triple_task = asyncio.create_task(_call_vertex_api(triple_payload))
        vertex_task = asyncio.create_task(_call_vertex_api(full_payload))
        
        triple_result, vertex_result = await asyncio.gather(triple_task, vertex_task)
        
        triple_text = triple_result['candidates'][0]['content']['parts'][0]['text']
        vertex_text = vertex_result['candidates'][0]['content']['parts'][0]['text']
        
        logger.info(json.dumps({
            "stage": "parallel_answers_generated",
            "triple_input": userPrompt,
            "triples_used": triples,
            "triple_answer_length": len(triple_text),
            "vertex_answer_length": len(vertex_text)
        }, ensure_ascii=False))

        # ğŸ”¹ Step 3&4: ìš”ì•½ê³¼ ê²€ì¦ì„ ë³‘ë ¬ ì²˜ë¦¬
        summary_payload = await _build_summary_payload(triple_text, vertex_text, userPrompt)
        
        summary_task = asyncio.create_task(_call_vertex_api(summary_payload))
        validation_task = asyncio.create_task(validate_response_relevance(userPrompt, f"{triple_text[:300]}..."))
        
        summary_result, is_relevant_preview = await asyncio.gather(summary_task, validation_task)
        summary_text = summary_result['candidates'][0]['content']['parts'][0]['text']
        
        # ìµœì¢… ê²€ì¦ (ìš”ì•½ ê²°ê³¼ ê¸°ì¤€)
        is_relevant = await validate_response_relevance(userPrompt, summary_text) if not is_relevant_preview else True
        
        if not is_relevant:
            logger.warning(f"ì‘ë‹µ ì—°ê´€ì„± ê²€ì¦ ì‹¤íŒ¨ - ì§ˆë¬¸: {userPrompt}")
            # ì²˜ìŒì„œë¹„ìŠ¤ì™€ ë¬´ê´€í•œ ì§ˆë¬¸ì— ëŒ€í•œ í‘œì¤€ ì‘ë‹µ
            summary_text = f"""ì£„ì†¡í•˜ì§€ë§Œ, **"{userPrompt}"**ì— ëŒ€í•œ ì •ë³´ëŠ” í˜„ì¬ ì œê³µí•´ë“œë¦¬ê¸° ì–´ë µìŠµë‹ˆë‹¤.

**ì²˜ìŒì„œë¹„ìŠ¤**ì˜ ì œí’ˆ ë° ì„œë¹„ìŠ¤ì— ê´€í•œ êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•´ì£¼ì‹œë©´, ë” ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì˜ˆë¥¼ ë“¤ì–´:
- íŠ¹ì • ê¸°ëŠ¥ì˜ ì‚¬ìš© ë°©ë²•
- ì„¤ì • ë° êµ¬ì„± ê´€ë ¨ ë¬¸ì˜  
- ë¬¸ì œ í•´ê²° ë°©ë²•
- ì„œë¹„ìŠ¤ ì´ìš© ê°€ì´ë“œ

ì¶”ê°€ ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“  ë¬¸ì˜í•´ ì£¼ì„¸ìš”! ğŸ˜Š"""

        logger.info(json.dumps({
            "stage": "summary_answer_generated",
            "user_prompt": userPrompt,
            "is_relevant": is_relevant,
            "summary_answer": summary_text[:200] + "..." if len(summary_text) > 200 else summary_text
        }, ensure_ascii=False))

        return JSONResponse({
            "triple_answer": triple_text,
            "vertex_answer": vertex_text,
            "summary_answer": summary_text,
            "updatedHistory": full_history,
            "quality_check": {
                "relevance_passed": is_relevant,
                "triples_found": len(triples) > 0
            }
        })

    except Exception as e:
        logger.exception("ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/gcs/{bucket_name}/{file_path:path}")
async def proxy_gcs_file(bucket_name: str, file_path: str):
    if not storage_client:
        raise HTTPException(status_code=503, detail="ìŠ¤í† ë¦¬ì§€ ì¸ì¦ ì‹¤íŒ¨")

    try:
        bucket = storage_client.bucket(bucket_name)
        blob = bucket.blob(file_path)
        if not blob.exists():
            raise HTTPException(status_code=404, detail="íŒŒì¼ ì—†ìŒ")

        def iterfile():
            with blob.open("rb") as f:
                yield from f

        content_type, _ = mimetypes.guess_type(file_path)
        content_type = content_type or "application/octet-stream"
        headers = {'Content-Disposition': f'inline; filename="{os.path.basename(file_path)}"'}
        return StreamingResponse(iterfile(), media_type=content_type, headers=headers)
    except Exception as e:
        logger.error("GCS í”„ë¡ì‹œ ì˜¤ë¥˜", exc_info=True)
        raise HTTPException(status_code=500, detail="íŒŒì¼ ì½ê¸° ì‹¤íŒ¨")

@app.get("/")
async def serve_root():
    return FileResponse("public/index.html")

app.mount("/", StaticFiles(directory="public"), name="static")

@app.get("/{full_path:path}")
async def serve_spa(full_path: str):
    if full_path.startswith("api") or os.path.exists(os.path.join("public", full_path)):
        raise HTTPException(status_code=404, detail="Not Found")
    return FileResponse("public/index.html")