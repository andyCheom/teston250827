"""í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ëª¨ë“ˆ"""
import re
import logging
from typing import List, Set, Dict
from functools import lru_cache

logger = logging.getLogger(__name__)

@lru_cache(maxsize=1)
def load_stopwords() -> Set[str]:
    stopwords = set()
    stopwords_path = "stopwords.txt"
    
    try:
        with open(stopwords_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    stopwords.add(line.lower())
        print(f"âœ… ë¶ˆìš©ì–´ {len(stopwords)}ê°œ ë¡œë“œ ì™„ë£Œ")
    except FileNotFoundError:
        print(f"âš ï¸ stopwords.txt íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {stopwords_path}")
    except Exception as e:
        print(f"âŒ ë¶ˆìš©ì–´ ë¡œë“œ ì˜¤ë¥˜: {e}")
    
    return stopwords

def clean_text(text: str) -> str:
    if not text:
        return ""
    cleaned = re.sub(r'[^\w\sã„±-ã…ã…-ã…£ê°€-í£]', ' ', text)
    cleaned = re.sub(r'\s+', ' ', cleaned)
    return cleaned.strip()

def extract_keywords(text: str, min_length: int = 2, max_keywords: int = 10) -> List[str]:
    if not text or not text.strip():
        return []
    
    stopwords = load_stopwords()
    cleaned_text = clean_text(text)
    tokens = cleaned_text.split()

    keywords = []
    seen = set()
    
    for token in tokens:
        token_lower = token.lower()
        if (len(token) >= min_length and token_lower not in stopwords and token_lower not in seen):
            keywords.append(token)
            seen.add(token_lower)
            if len(keywords) >= max_keywords:
                break
    
    return keywords

def get_search_terms(user_prompt: str) -> List[str]:
    normalized_prompt = normalize_text_for_search(user_prompt)
    
    if normalized_prompt != user_prompt:
        print(f"ğŸ”§ í…ìŠ¤íŠ¸ ì •ê·œí™”: '{user_prompt}' â†’ '{normalized_prompt}'")
    
    keywords = extract_keywords(normalized_prompt, min_length=2, max_keywords=8)
    if len(keywords) < 2:
        keywords = extract_keywords(normalized_prompt, min_length=1, max_keywords=8)
    if not keywords:
        keywords = [normalized_prompt.strip()] if normalized_prompt.strip() else [user_prompt.strip()]
    
    return keywords

@lru_cache(maxsize=100)
def get_typo_correction_patterns() -> Dict[str, str]:
    return {
        r'ì–´ë–¡ê²Œ': 'ì–´ë–»ê²Œ',
        r'ì–´ë–»í•´': 'ì–´ë–»ê²Œ', 
        r'ì–´ìº': 'ì–´ë–»ê²Œ',
        r'ì–´ëœ¨ì¼€': 'ì–´ë–»ê²Œ',
        r'ì–´ë–»ê°œ': 'ì–´ë–»ê²Œ',
        r'ì•Šë¨': 'ì•ˆë¼',
        r'ì•ˆë¨': 'ì•ˆë¼',
        r'ë˜ìš©': 'ë˜ìš”',
        r'ë˜ì—¬': 'ë˜ì–´',
        r'í•´ì—¬': 'í•´ì„œ',
        r'ì´ì¨': 'ìˆì–´',
        r'ì—…ì¨': 'ì—†ì–´',
        r'ì‡ì–´': 'ìˆì–´',
        r'ì—…ì–´': 'ì—†ì–´',
        r'ì‚¬ìš©ë°©ë»': 'ì‚¬ìš©ë°©ë²•',
        r'ì‚¬ìš©ë°¥ë²•': 'ì‚¬ìš©ë°©ë²•',
        r'ì‚¬ìš©ë²•': 'ì‚¬ìš©ë°©ë²•',
        r'ë¨¸ì•¼': 'ë­ì•¼',
        r'ëª¨ì•¼': 'ë­ì•¼',
        r'ë§ˆì´ë§¤ì¼ëŸ¬': 'ë§ˆì´ë©”ì¼ëŸ¬',
        r'ë§¤ì¼ëŸ¬': 'ë©”ì¼ëŸ¬',
        r'ë¡œê¸´': 'ë¡œê·¸ì¸',
        r'ë¡œê·¸ì¸': 'ë¡œê·¸ì¸',
        r'íŒ¨ìŠ¤ì›Œë“œ': 'ë¹„ë°€ë²ˆí˜¸',
        r'ë¹„ë²ˆ': 'ë¹„ë°€ë²ˆí˜¸',
        r'ì„¤ì¹˜': 'ì„¤ì¹˜',
        r'ì…‹íŒ…': 'ì„¤ì •',
        r'ì„¸íŒ…': 'ì„¤ì •',
        r'ë‹¤ìš´ë¡œë“œ': 'ë‹¤ìš´ë¡œë“œ',
        r'ì—…ë¡œë“œ': 'ì—…ë¡œë“œ',
        r'ì‚­ì¬': 'ì‚­ì œ',
        r'\bteh\b': 'the',
        r'\bamd\b': 'and',
        r'\byuo\b': 'you', 
        r'\btaht\b': 'that',
        r'\bform\b': 'from',
        r'\bwith\b': 'with',
        r'([ã…‹ã…]){3,}': r'\1\1',
        r'([a-zA-Z])\1{2,}': r'\1',
        r'([!?.])\1{2,}': r'\1',
        r'ã…œ{2,}': 'ã…œ',
        r'ã… {2,}': 'ã… ',
        r'([ã…-ã…£ã„±-ã…])\1{1,}': r'\1',
        r'[ã…-ã…£ã„±-ã…]{2,}': '' 
    }

def fix_typos(text: str) -> str:
    if not text or not text.strip():
        return text

    patterns = get_typo_correction_patterns()
    corrected_text = text

    protected_terms = {
        'ì²˜ìŒì„œë¹„ìŠ¤', 'ì²˜ìŒì†Œí”„íŠ¸', 'ì”¨ë””ì— ì†Œí”„íŠ¸', 'ì²˜ìŒì„œë² ì´',
        'API', 'UI', 'UX', 'DB', 'URL', 'IP', 'ID', 'GraphRAG'
    }
    term_placeholders = {}
    for i, term in enumerate(protected_terms):
        if term in corrected_text:
            placeholder = f"__PROTECTED_TERM_{i}__"
            term_placeholders[placeholder] = term
            corrected_text = corrected_text.replace(term, placeholder)

    for pattern, replacement in patterns.items():
        try:
            corrected_text = re.sub(pattern, replacement, corrected_text, flags=re.IGNORECASE)
        except re.error:
            continue

    for placeholder, term in term_placeholders.items():
        corrected_text = corrected_text.replace(placeholder, term)

    corrected_text = re.sub(r'\s+', ' ', corrected_text).strip()
    
    return corrected_text

def normalize_text_for_search(text: str) -> str:
    if not text:
        return ""
    
    spacing_corrected = basic_spacing_rules(text)
    typo_corrected = fix_typos(spacing_corrected)
    normalized = re.sub(r'[^\w\sê°€-í£ã„±-ã…ã…-ã…£]', ' ', typo_corrected)
    normalized = re.sub(r'\s+', ' ', normalized).strip()
    
    return normalized

def basic_spacing_rules(text: str) -> str:
    """ê°„ë‹¨í•œ ë„ì–´ì“°ê¸° ê·œì¹™ë§Œ ì ìš© (PyKoSpacing ì œê±° ë²„ì „)"""
    corrected = text

    spacing_patterns = {
        r'([ê°€-í£]) (ì´ì—ìš”|ì˜ˆìš”|ì…ë‹ˆë‹¤|ìŠµë‹ˆë‹¤|ì—ìš”|ì´ì•¼)': r'\1\2',
        r'ë¡œê·¸ ?ì¸': 'ë¡œê·¸ì¸',
        r'ë¹„ë°€ ?ë²ˆí˜¸': 'ë¹„ë°€ë²ˆí˜¸',
        r'ì‚¬ìš© ?ë°©ë²•': 'ì‚¬ìš©ë°©ë²•',
        r'ì²˜ìŒ ?ì„œë¹„ìŠ¤': 'ì²˜ìŒì„œë¹„ìŠ¤',
        r'ë§ˆì´ ?ë©”ì¼ëŸ¬': 'ë§ˆì´ë©”ì¼ëŸ¬',
        r'ì–´ë–» ?ê²Œ': 'ì–´ë–»ê²Œ',
        r'ë¬´ì—‡ ?ì„': 'ë¬´ì—‡ì„',
        r'ì–¸ì œ ?ë¶€í„°': 'ì–¸ì œë¶€í„°',
        r'ì–´ë”” ?ì„œ': 'ì–´ë””ì„œ',
        r'ë°ì´í„° ?ë² ì´ìŠ¤': 'ë°ì´í„°ë² ì´ìŠ¤',
        r'í™ˆ ?í˜ì´ì§€': 'í™ˆí˜ì´ì§€',
        r'ì›¹ ?ì‚¬ì´íŠ¸': 'ì›¹ì‚¬ì´íŠ¸',
        r'íŒŒì¼ ?ì—…ë¡œë“œ': 'íŒŒì¼ì—…ë¡œë“œ',
        r'ë‹¤ìš´ ?ë¡œë“œ': 'ë‹¤ìš´ë¡œë“œ',
        r'í•  ?ìˆ˜ ?ìˆë‹¤': 'í• ìˆ˜ìˆë‹¤',
        r'í•  ?ìˆ˜ ?ì—†ë‹¤': 'í• ìˆ˜ì—†ë‹¤',
        r'ë˜ì§€ ?ì•ŠëŠ”ë‹¤': 'ë˜ì§€ì•ŠëŠ”ë‹¤',
    }

    for pattern, replacement in spacing_patterns.items():
        corrected = re.sub(pattern, replacement, corrected)
    
    return corrected
